{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEEpUvWFK4IZVI68Qis6So",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sammainahkinya1404/Programming-Reboot/blob/main/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlAvZX_GikJ3"
      },
      "outputs": [],
      "source": [
        "# Basic RL Interaction Loop)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Pseudo code for RL loop\n",
        "# while True:\n",
        "#     current_state = environment.get_state()\n",
        "#     action = agent.choose_action(current_state)\n",
        "#     next_state, reward = environment.step(action)\n",
        "#     agent.update(current_state, action, reward, next_state)\n",
        "#     if environment.is_done():\n",
        "#         break\n"
      ],
      "metadata": {
        "id": "ofJEbX5ei676"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting Up a Basic RL Environment"
      ],
      "metadata": {
        "id": "4PlPLHBRkg0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simple environment setup\n",
        "states = ['A', 'B', 'C', 'D']\n",
        "actions = ['left', 'right']\n",
        "reward_table = {'A': {'left': -1, 'right': 1}, 'B': {'left': 0, 'right': 2}}\n",
        "\n",
        "# Example state-action-reward interaction\n",
        "current_state = 'A'\n",
        "action = 'right'\n",
        "reward = reward_table[current_state][action]\n",
        "print(f\"State: {current_state}, Action: {action}, Reward: {reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em2qYMCikiFI",
        "outputId": "ed5ac758-7063-4b2b-9231-833673207ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: A, Action: right, Reward: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "num_states = 4\n",
        "num_actions = 2\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.2  # Exploration rate\n",
        "\n",
        "# Q-Learning update\n",
        "state, action, reward, next_state = 0, 1, 1, 2  # Example values\n",
        "Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "print(\"Updated Q-Table:\", Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_5tHpyQmmrm",
        "outputId": "9508e4e0-3012-4caf-8e74-bea432e70c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Q-Table: [[0.  0.1]\n",
            " [0.  0. ]\n",
            " [0.  0. ]\n",
            " [0.  0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple policy network\n",
        "model = Sequential([\n",
        "    Dense(24, input_dim=4, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(2, activation='softmax')  # Two actions\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='categorical_crossentropy')\n",
        "\n",
        "# Example: Training step with fake data\n",
        "state = np.array([[1, 0, 0, 1]])  # Example state\n",
        "action_prob = model.predict(state)\n",
        "print(\"Action Probabilities:\", action_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Aibz_ZoY9e",
        "outputId": "acd02be2-5fe2-4dbb-96a0-8c71bb002d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "Action Probabilities: [[0.50306433 0.49693564]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Initialize variables\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "\n",
        "# Run one episode\n",
        "done = False\n",
        "while not done:\n",
        "    env.render()  # Display the environment\n",
        "    action = env.action_space.sample()  # Random action\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "env.close()\n",
        "print(\"Total Reward:\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzBl8EXeqM_L",
        "outputId": "69ceb9a2-b497-4e8c-da36-8fddf3f72daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: 36.0\n"
          ]
        }
      ]
    }
  ]
}